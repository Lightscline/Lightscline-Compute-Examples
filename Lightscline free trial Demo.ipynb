{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a818ae7",
   "metadata": {},
   "source": [
    "### TLDR:\n",
    "\n",
    "1. Download and install the software. \n",
    "2. Import your data\n",
    "5. Train your model \n",
    "6. See the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51e1d0",
   "metadata": {},
   "source": [
    "## 1. Download and Install the software\n",
    "\n",
    "### 1.1 Download the software \n",
    "\n",
    "\n",
    "1. Register yourself via <a href=\"https://forms.gle/ND5hFbznqmyVQKaR6\" target=\"_blank\">google form</a>.\n",
    "2. Within 24 hrs, You will get the link to download our software free for 21 days (no credit card required)\n",
    "\n",
    "### 1.2 Install our software\n",
    "\n",
    "Our current software only works Windowsx64 with **python 3.9, 3.10 and 3.11** Please contact us for any more version support.\n",
    "\n",
    "0. (optional) Create a virtual environment \n",
    "\n",
    " - run `python -m venv env` to create a new virtual environment. This should create a folder named env.\n",
    " - run `.\\env\\Scripts\\activate` to activate the virtual environment.\n",
    " - (optional) install jupyter notebook for iteractive session `pip install jupyter`\n",
    " \n",
    "1. Download and install Microsoft Visual C++ Redistributable: https://aka.ms/vs/16/release/vc_redist.x64.exe  \n",
    "2. Install lightscline package by `pip install <path to the .whl file>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d250bf",
   "metadata": {},
   "source": [
    "### 1.3 Import the package to check if it is correctly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79bab9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a free version with limited capabilities.\n",
      "Please contact us at info@lightscline.com for the full version. Our team will guide you through the complete suite of functionalities, tailored for your specific needs.\n"
     ]
    }
   ],
   "source": [
    "from lightscline.lightscline import LightsclineEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fec7e",
   "metadata": {},
   "source": [
    "## 2. Import your data\n",
    "\n",
    "You can either import your data or use a dummy dataset to see how it work. \n",
    "\n",
    "Dummy data can be created by typing `data = [[0 for i in range(10000)], [1 for i in range(8000)]]` in python. This will create 2 classes ( two timeseries columns of unequal lengths). Each column represents one class, unless speficified otherwise. Below is a example for getting the data into classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3930941",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation for ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526b411",
   "metadata": {},
   "source": [
    "This example uses \n",
    "<a href=\"https://engineering.case.edu/bearingdatacenter\" target=\"_blank\">Case Western Reserve University bearing fault dataset</a>. The data files are present in the `data` folder <a href=\"https://github.com/Lightscline/LightsclineEdge-Demo/tree/main/data\" target=\"_blank\"> here <a>.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cbea4",
   "metadata": {},
   "source": [
    "    \n",
    "Each of the .mat files are 4 different classes (3 different types of bearing fault + 1 normal bearing). The data for each class is a time series vibration data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f33fa",
   "metadata": {},
   "source": [
    "You can also use your own dataset here instead of using the example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe0b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify where the data is located. \n",
    "data_folder = r\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfcc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "## load the data\n",
    "mat1 = scipy.io.loadmat(f'{data_folder}/Normal_1730_1.mat')\n",
    "mat2 = scipy.io.loadmat(f'{data_folder}/B007_1_DE.mat')\n",
    "mat3 = scipy.io.loadmat(f'{data_folder}/IR007_1_DE.mat')\n",
    "mat4 = scipy.io.loadmat(f'{data_folder}/OR007_1_DE.mat')\n",
    "\n",
    "## Convert the data of each file into a list \n",
    "normal_data = mat1['X100_DE_time'].reshape(-1).tolist()\n",
    "ball_defect = mat2['X121_DE_time'].reshape(-1).tolist()\n",
    "IR_defect = mat3['X108_DE_time'].reshape(-1).tolist()\n",
    "OR_defect = mat4['X133_DE_time'].reshape(-1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4404041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the data into desired format.\n",
    "data = [normal_data[:100000],ball_defect[:100000], IR_defect[:100000], OR_defect[:100000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627dc9c",
   "metadata": {},
   "source": [
    "### 2.2 Ingest your data into LightsclineEdge package\n",
    "\n",
    "Once the dataset is in form of List of List, we can import it into LightsclineEdge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feea6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LightsclineEdge(data=data,fs = 12000,labels = None)  ## Labels will be decided based on column number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f1d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will create windows of 4000<window_size> points each and reduce each window size by 90% to train the data on trained windown size.\n",
    "ls.reduce_and_preprocess_data(per_reduction=90) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42513e",
   "metadata": {},
   "source": [
    "## 3. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e284da",
   "metadata": {},
   "source": [
    "You can change the number of layers to find the best model for your data. More advanced finetuning can be done by contacting us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a080e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 loss:  1.3804817199707031\n",
      "epoch:  10 loss:  1.3512498140335083\n",
      "epoch:  15 loss:  1.324153184890747\n",
      "epoch:  20 loss:  1.2961316108703613\n",
      "epoch:  25 loss:  1.2670230865478516\n",
      "epoch:  30 loss:  1.2373204231262207\n",
      "epoch:  35 loss:  1.2062278985977173\n",
      "epoch:  40 loss:  1.173331379890442\n",
      "epoch:  45 loss:  1.1390955448150635\n",
      "epoch:  50 loss:  1.1044927835464478\n",
      "epoch:  55 loss:  1.0704748630523682\n",
      "epoch:  60 loss:  1.037457823753357\n",
      "epoch:  65 loss:  1.0055168867111206\n",
      "epoch:  70 loss:  0.9746091961860657\n",
      "epoch:  75 loss:  0.9446586966514587\n",
      "epoch:  80 loss:  0.9155473709106445\n",
      "epoch:  85 loss:  0.8871724009513855\n",
      "epoch:  90 loss:  0.8594869375228882\n",
      "epoch:  95 loss:  0.8325103521347046\n",
      "epoch:  100 loss:  0.8062198758125305\n",
      "epoch:  105 loss:  0.7805963158607483\n",
      "epoch:  110 loss:  0.7557184100151062\n",
      "epoch:  115 loss:  0.7316595315933228\n",
      "epoch:  120 loss:  0.7083501815795898\n",
      "epoch:  125 loss:  0.6857761740684509\n",
      "epoch:  130 loss:  0.6639025807380676\n",
      "epoch:  135 loss:  0.6427448391914368\n",
      "epoch:  140 loss:  0.6222849488258362\n",
      "epoch:  145 loss:  0.6024277806282043\n",
      "epoch:  150 loss:  0.5830249786376953\n",
      "epoch:  155 loss:  0.5641632676124573\n",
      "epoch:  160 loss:  0.5459517240524292\n",
      "epoch:  165 loss:  0.5282592177391052\n",
      "epoch:  170 loss:  0.5110653638839722\n",
      "epoch:  175 loss:  0.494590163230896\n",
      "epoch:  180 loss:  0.4787609279155731\n",
      "epoch:  185 loss:  0.4634968340396881\n",
      "epoch:  190 loss:  0.4487198293209076\n",
      "epoch:  195 loss:  0.43450266122817993\n",
      "epoch:  200 loss:  0.4209112823009491\n",
      "epoch:  205 loss:  0.4078470766544342\n",
      "epoch:  210 loss:  0.3953629434108734\n",
      "epoch:  215 loss:  0.38334420323371887\n",
      "epoch:  220 loss:  0.37181025743484497\n",
      "epoch:  225 loss:  0.36074206233024597\n",
      "epoch:  230 loss:  0.3501753807067871\n",
      "epoch:  235 loss:  0.3400964140892029\n",
      "epoch:  240 loss:  0.3304207921028137\n",
      "epoch:  245 loss:  0.3211154639720917\n",
      "epoch:  250 loss:  0.3121410310268402\n",
      "epoch:  255 loss:  0.3034955561161041\n",
      "epoch:  260 loss:  0.2951525151729584\n",
      "epoch:  265 loss:  0.2870624363422394\n",
      "epoch:  270 loss:  0.2792290449142456\n",
      "epoch:  275 loss:  0.27165064215660095\n",
      "epoch:  280 loss:  0.26429906487464905\n",
      "epoch:  285 loss:  0.257188081741333\n",
      "epoch:  290 loss:  0.25035062432289124\n",
      "epoch:  295 loss:  0.24377883970737457\n",
      "epoch:  300 loss:  0.2374584972858429\n",
      "epoch:  305 loss:  0.23137421905994415\n",
      "epoch:  310 loss:  0.22552549839019775\n",
      "epoch:  315 loss:  0.21992109715938568\n",
      "epoch:  320 loss:  0.2145448625087738\n",
      "epoch:  325 loss:  0.20938052237033844\n",
      "epoch:  330 loss:  0.20441533625125885\n",
      "epoch:  335 loss:  0.1996341049671173\n",
      "epoch:  340 loss:  0.19502520561218262\n",
      "epoch:  345 loss:  0.1905793994665146\n",
      "epoch:  350 loss:  0.18628574907779694\n",
      "epoch:  355 loss:  0.18213187158107758\n",
      "epoch:  360 loss:  0.17810769379138947\n",
      "epoch:  365 loss:  0.17421294748783112\n",
      "epoch:  370 loss:  0.17044593393802643\n",
      "epoch:  375 loss:  0.1667952835559845\n",
      "epoch:  380 loss:  0.16325005888938904\n",
      "epoch:  385 loss:  0.15980637073516846\n",
      "epoch:  390 loss:  0.15645968914031982\n",
      "epoch:  395 loss:  0.15320660173892975\n",
      "epoch:  400 loss:  0.1500469446182251\n",
      "epoch:  405 loss:  0.14697451889514923\n",
      "epoch:  410 loss:  0.1439831703901291\n",
      "epoch:  415 loss:  0.14106927812099457\n",
      "epoch:  420 loss:  0.1382312923669815\n",
      "epoch:  425 loss:  0.13546712696552277\n",
      "epoch:  430 loss:  0.13277314603328705\n",
      "epoch:  435 loss:  0.13015110790729523\n",
      "epoch:  440 loss:  0.12759456038475037\n",
      "epoch:  445 loss:  0.12510237097740173\n",
      "epoch:  450 loss:  0.12270092219114304\n",
      "epoch:  455 loss:  0.12037426978349686\n",
      "epoch:  460 loss:  0.11811497062444687\n",
      "epoch:  465 loss:  0.11591682583093643\n",
      "epoch:  470 loss:  0.11378465592861176\n",
      "epoch:  475 loss:  0.11171562224626541\n",
      "epoch:  480 loss:  0.10970934480428696\n",
      "epoch:  485 loss:  0.10776346921920776\n",
      "epoch:  490 loss:  0.10587316751480103\n",
      "epoch:  495 loss:  0.10403434932231903\n",
      "epoch:  500 loss:  0.10224516689777374\n",
      "epoch:  505 loss:  0.10050494223833084\n",
      "epoch:  510 loss:  0.09881346672773361\n",
      "epoch:  515 loss:  0.09716834872961044\n",
      "epoch:  520 loss:  0.09556633234024048\n",
      "epoch:  525 loss:  0.09400654584169388\n",
      "epoch:  530 loss:  0.09249070286750793\n",
      "epoch:  535 loss:  0.09101888537406921\n",
      "epoch:  540 loss:  0.0895896926522255\n",
      "epoch:  545 loss:  0.08820074796676636\n",
      "epoch:  550 loss:  0.08685004711151123\n",
      "epoch:  555 loss:  0.08553636819124222\n",
      "epoch:  560 loss:  0.08425842970609665\n",
      "epoch:  565 loss:  0.0830145850777626\n",
      "epoch:  570 loss:  0.08180304616689682\n",
      "epoch:  575 loss:  0.08062215894460678\n",
      "epoch:  580 loss:  0.07947047054767609\n",
      "epoch:  585 loss:  0.0783466100692749\n",
      "epoch:  590 loss:  0.07724891602993011\n",
      "epoch:  595 loss:  0.07617548108100891\n",
      "epoch:  600 loss:  0.07512497901916504\n",
      "epoch:  605 loss:  0.07409963011741638\n",
      "epoch:  610 loss:  0.07310231029987335\n",
      "epoch:  615 loss:  0.07213214784860611\n",
      "epoch:  620 loss:  0.07118672877550125\n",
      "epoch:  625 loss:  0.07026371359825134\n",
      "epoch:  630 loss:  0.0693613588809967\n",
      "epoch:  635 loss:  0.06847851723432541\n",
      "epoch:  640 loss:  0.0676141157746315\n",
      "epoch:  645 loss:  0.06676723062992096\n",
      "epoch:  650 loss:  0.06593717634677887\n",
      "epoch:  655 loss:  0.06512381881475449\n",
      "epoch:  660 loss:  0.06432771682739258\n",
      "epoch:  665 loss:  0.06354918330907822\n",
      "epoch:  670 loss:  0.06278777867555618\n",
      "epoch:  675 loss:  0.06204281747341156\n",
      "epoch:  680 loss:  0.061313752084970474\n",
      "epoch:  685 loss:  0.060600217431783676\n",
      "epoch:  690 loss:  0.05990194156765938\n",
      "epoch:  695 loss:  0.059218715876340866\n",
      "epoch:  700 loss:  0.05855036526918411\n",
      "epoch:  705 loss:  0.05789671838283539\n",
      "epoch:  710 loss:  0.05725736916065216\n",
      "epoch:  715 loss:  0.05663172900676727\n",
      "epoch:  720 loss:  0.056019127368927\n",
      "epoch:  725 loss:  0.05541887879371643\n",
      "epoch:  730 loss:  0.054830312728881836\n",
      "epoch:  735 loss:  0.05425277352333069\n",
      "epoch:  740 loss:  0.053685639053583145\n",
      "epoch:  745 loss:  0.053128428757190704\n",
      "epoch:  750 loss:  0.05258098244667053\n",
      "epoch:  755 loss:  0.05204344168305397\n",
      "epoch:  760 loss:  0.051516029983758926\n",
      "epoch:  765 loss:  0.0509987436234951\n",
      "epoch:  770 loss:  0.05049124360084534\n",
      "epoch:  775 loss:  0.049992941319942474\n",
      "epoch:  780 loss:  0.049503132700920105\n",
      "epoch:  785 loss:  0.04902111738920212\n",
      "epoch:  790 loss:  0.04854638874530792\n",
      "epoch:  795 loss:  0.04807880148291588\n",
      "epoch:  800 loss:  0.047618620097637177\n",
      "epoch:  805 loss:  0.047166045755147934\n",
      "epoch:  810 loss:  0.04672083631157875\n",
      "epoch:  815 loss:  0.046282488852739334\n",
      "epoch:  820 loss:  0.04585053771734238\n",
      "epoch:  825 loss:  0.045424848794937134\n",
      "epoch:  830 loss:  0.04500570520758629\n",
      "epoch:  835 loss:  0.0445934422314167\n",
      "epoch:  840 loss:  0.04418808966875076\n",
      "epoch:  845 loss:  0.043789468705654144\n",
      "epoch:  850 loss:  0.04339747503399849\n",
      "epoch:  855 loss:  0.043012067675590515\n",
      "epoch:  860 loss:  0.04263302683830261\n",
      "epoch:  865 loss:  0.04225999861955643\n",
      "epoch:  870 loss:  0.041892845183610916\n",
      "epoch:  875 loss:  0.04153143987059593\n",
      "epoch:  880 loss:  0.04117562621831894\n",
      "epoch:  885 loss:  0.04082532227039337\n",
      "epoch:  890 loss:  0.04048057645559311\n",
      "epoch:  895 loss:  0.04014149308204651\n",
      "epoch:  900 loss:  0.03980804979801178\n",
      "epoch:  905 loss:  0.03948022797703743\n",
      "epoch:  910 loss:  0.039157934486866\n",
      "epoch:  915 loss:  0.038841065019369125\n",
      "epoch:  920 loss:  0.03852950781583786\n",
      "epoch:  925 loss:  0.0382232740521431\n",
      "epoch:  930 loss:  0.03792240470647812\n",
      "epoch:  935 loss:  0.037626851350069046\n",
      "epoch:  940 loss:  0.03733640909194946\n",
      "epoch:  945 loss:  0.03705080226063728\n",
      "epoch:  950 loss:  0.0367698036134243\n",
      "epoch:  955 loss:  0.036493249237537384\n",
      "epoch:  960 loss:  0.036221012473106384\n",
      "epoch:  965 loss:  0.035953033715486526\n",
      "epoch:  970 loss:  0.035689253360033035\n",
      "epoch:  975 loss:  0.03542961925268173\n",
      "epoch:  980 loss:  0.03517405316233635\n",
      "epoch:  985 loss:  0.03492245823144913\n",
      "epoch:  990 loss:  0.0346747450530529\n",
      "epoch:  995 loss:  0.03443080931901932\n",
      "epoch:  1000 loss:  0.03419053182005882\n",
      "epoch:  1005 loss:  0.03395380824804306\n",
      "epoch:  1010 loss:  0.03372052684426308\n",
      "epoch:  1015 loss:  0.03349059820175171\n",
      "epoch:  1020 loss:  0.0332639217376709\n",
      "epoch:  1025 loss:  0.033040422946214676\n",
      "epoch:  1030 loss:  0.032820023596286774\n",
      "epoch:  1035 loss:  0.03260266035795212\n",
      "epoch:  1040 loss:  0.032388266175985336\n",
      "epoch:  1045 loss:  0.03217677026987076\n",
      "epoch:  1050 loss:  0.03196810185909271\n",
      "epoch:  1055 loss:  0.03176218271255493\n",
      "epoch:  1060 loss:  0.03155892342329025\n",
      "epoch:  1065 loss:  0.031358253210783005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1070 loss:  0.031160086393356323\n",
      "epoch:  1075 loss:  0.03096434660255909\n",
      "epoch:  1080 loss:  0.03077094443142414\n",
      "epoch:  1085 loss:  0.03057960979640484\n",
      "epoch:  1090 loss:  0.030389389023184776\n",
      "epoch:  1095 loss:  0.030198099091649055\n",
      "epoch:  1100 loss:  0.03001103363931179\n",
      "epoch:  1105 loss:  0.029827240854501724\n",
      "epoch:  1110 loss:  0.029645763337612152\n",
      "epoch:  1115 loss:  0.029466427862644196\n",
      "epoch:  1120 loss:  0.029289912432432175\n",
      "epoch:  1125 loss:  0.029115673154592514\n",
      "epoch:  1130 loss:  0.028943456709384918\n",
      "epoch:  1135 loss:  0.02877357229590416\n",
      "epoch:  1140 loss:  0.0286061130464077\n",
      "epoch:  1145 loss:  0.028440840542316437\n",
      "epoch:  1150 loss:  0.02827778086066246\n",
      "epoch:  1155 loss:  0.02811679057776928\n",
      "epoch:  1160 loss:  0.027957824990153313\n",
      "epoch:  1165 loss:  0.027800865471363068\n",
      "epoch:  1170 loss:  0.02764584869146347\n",
      "epoch:  1175 loss:  0.027492672204971313\n",
      "epoch:  1180 loss:  0.027341291308403015\n",
      "epoch:  1185 loss:  0.02719164825975895\n",
      "epoch:  1190 loss:  0.0270436629652977\n",
      "epoch:  1195 loss:  0.026897301897406578\n",
      "epoch:  1200 loss:  0.026752492412924767\n",
      "epoch:  1205 loss:  0.026609184220433235\n",
      "epoch:  1210 loss:  0.026467353105545044\n",
      "epoch:  1215 loss:  0.026326995342969894\n",
      "epoch:  1220 loss:  0.026188116520643234\n",
      "epoch:  1225 loss:  0.026050716638565063\n",
      "epoch:  1230 loss:  0.02591477520763874\n",
      "epoch:  1235 loss:  0.02578027732670307\n",
      "epoch:  1240 loss:  0.025647178292274475\n",
      "epoch:  1245 loss:  0.025515444576740265\n",
      "epoch:  1250 loss:  0.025385024026036263\n",
      "epoch:  1255 loss:  0.025255875661969185\n",
      "epoch:  1260 loss:  0.025127951055765152\n",
      "epoch:  1265 loss:  0.025001222267746925\n",
      "epoch:  1270 loss:  0.02487567439675331\n",
      "epoch:  1275 loss:  0.024751318618655205\n",
      "epoch:  1280 loss:  0.024628175422549248\n",
      "epoch:  1285 loss:  0.024506274610757828\n",
      "epoch:  1290 loss:  0.024385616183280945\n",
      "epoch:  1295 loss:  0.02426615171134472\n",
      "epoch:  1300 loss:  0.024147752672433853\n",
      "epoch:  1305 loss:  0.024030271917581558\n",
      "epoch:  1310 loss:  0.023913657292723656\n",
      "epoch:  1315 loss:  0.023797614499926567\n",
      "epoch:  1320 loss:  0.023681065067648888\n",
      "epoch:  1325 loss:  0.023565378040075302\n",
      "epoch:  1330 loss:  0.023453714326024055\n",
      "epoch:  1335 loss:  0.023344596847891808\n",
      "epoch:  1340 loss:  0.023237116634845734\n",
      "epoch:  1345 loss:  0.023130694404244423\n",
      "epoch:  1350 loss:  0.02302497997879982\n",
      "epoch:  1355 loss:  0.02292010188102722\n",
      "epoch:  1360 loss:  0.022816529497504234\n",
      "epoch:  1365 loss:  0.02271432615816593\n",
      "epoch:  1370 loss:  0.02261313609778881\n",
      "epoch:  1375 loss:  0.02251277305185795\n",
      "epoch:  1380 loss:  0.022413220256567\n",
      "epoch:  1385 loss:  0.02231443114578724\n",
      "epoch:  1390 loss:  0.022216394543647766\n",
      "epoch:  1395 loss:  0.02211916819214821\n",
      "epoch:  1400 loss:  0.02202281914651394\n",
      "epoch:  1405 loss:  0.02192738465964794\n",
      "epoch:  1410 loss:  0.02183290757238865\n",
      "epoch:  1415 loss:  0.021739395335316658\n",
      "epoch:  1420 loss:  0.021646806970238686\n",
      "epoch:  1425 loss:  0.021555105224251747\n",
      "epoch:  1430 loss:  0.021464282646775246\n",
      "epoch:  1435 loss:  0.021374382078647614\n",
      "epoch:  1440 loss:  0.021285459399223328\n",
      "epoch:  1445 loss:  0.021197574213147163\n",
      "epoch:  1450 loss:  0.021110760048031807\n",
      "epoch:  1455 loss:  0.021025052294135094\n",
      "epoch:  1460 loss:  0.020940428599715233\n",
      "epoch:  1465 loss:  0.020856866613030434\n",
      "epoch:  1470 loss:  0.02077430486679077\n",
      "epoch:  1475 loss:  0.02069271169602871\n",
      "epoch:  1480 loss:  0.020612021908164024\n",
      "epoch:  1485 loss:  0.02053219825029373\n",
      "epoch:  1490 loss:  0.020453205332159996\n",
      "epoch:  1495 loss:  0.02037501148879528\n",
      "epoch:  1500 loss:  0.020297586917877197\n",
      "epoch:  1505 loss:  0.020220911130309105\n",
      "epoch:  1510 loss:  0.02014497108757496\n",
      "epoch:  1515 loss:  0.020069735124707222\n",
      "epoch:  1520 loss:  0.019995201379060745\n",
      "epoch:  1525 loss:  0.019921347498893738\n",
      "epoch:  1530 loss:  0.01984817162156105\n",
      "epoch:  1535 loss:  0.01977565512061119\n",
      "epoch:  1540 loss:  0.019703790545463562\n",
      "epoch:  1545 loss:  0.019632572308182716\n",
      "epoch:  1550 loss:  0.019561992958188057\n",
      "epoch:  1555 loss:  0.019492046907544136\n",
      "epoch:  1560 loss:  0.01942272111773491\n",
      "epoch:  1565 loss:  0.01935400813817978\n",
      "epoch:  1570 loss:  0.019285893067717552\n",
      "epoch:  1575 loss:  0.019218377768993378\n",
      "epoch:  1580 loss:  0.019151443615555763\n",
      "epoch:  1585 loss:  0.01908508688211441\n",
      "epoch:  1590 loss:  0.019019300118088722\n",
      "epoch:  1595 loss:  0.018954074010252953\n",
      "epoch:  1600 loss:  0.0188894085586071\n",
      "epoch:  1605 loss:  0.018825286999344826\n",
      "epoch:  1610 loss:  0.018761716783046722\n",
      "epoch:  1615 loss:  0.018698692321777344\n",
      "epoch:  1620 loss:  0.018636198714375496\n",
      "epoch:  1625 loss:  0.018574239686131477\n",
      "epoch:  1630 loss:  0.018512804061174393\n",
      "epoch:  1635 loss:  0.018451889976859093\n",
      "epoch:  1640 loss:  0.018391495570540428\n",
      "epoch:  1645 loss:  0.018331600353121758\n",
      "epoch:  1650 loss:  0.018272213637828827\n",
      "epoch:  1655 loss:  0.018213313072919846\n",
      "epoch:  1660 loss:  0.01815488561987877\n",
      "epoch:  1665 loss:  0.018096907064318657\n",
      "epoch:  1670 loss:  0.018039317801594734\n",
      "epoch:  1675 loss:  0.01798207126557827\n",
      "epoch:  1680 loss:  0.01792515628039837\n",
      "epoch:  1685 loss:  0.017868630588054657\n",
      "epoch:  1690 loss:  0.017812564969062805\n",
      "epoch:  1695 loss:  0.01775696873664856\n",
      "epoch:  1700 loss:  0.017701812088489532\n",
      "epoch:  1705 loss:  0.017647068947553635\n",
      "epoch:  1710 loss:  0.017592716962099075\n",
      "epoch:  1715 loss:  0.017538754269480705\n",
      "epoch:  1720 loss:  0.01748521998524666\n",
      "epoch:  1725 loss:  0.01743215136229992\n",
      "epoch:  1730 loss:  0.017379580065608025\n",
      "epoch:  1735 loss:  0.017327522858977318\n",
      "epoch:  1740 loss:  0.01727597415447235\n",
      "epoch:  1745 loss:  0.017224913462996483\n",
      "epoch:  1750 loss:  0.017174316570162773\n",
      "epoch:  1755 loss:  0.017124174162745476\n",
      "epoch:  1760 loss:  0.017074601724743843\n",
      "epoch:  1765 loss:  0.017028018832206726\n",
      "epoch:  1770 loss:  0.016976315528154373\n",
      "epoch:  1775 loss:  0.016927948221564293\n",
      "epoch:  1780 loss:  0.016879618167877197\n",
      "epoch:  1785 loss:  0.016831832006573677\n",
      "epoch:  1790 loss:  0.01678452081978321\n",
      "epoch:  1795 loss:  0.01673758029937744\n",
      "epoch:  1800 loss:  0.016691040247678757\n",
      "epoch:  1805 loss:  0.0166447963565588\n",
      "epoch:  1810 loss:  0.01659887656569481\n",
      "epoch:  1815 loss:  0.016553262248635292\n",
      "epoch:  1820 loss:  0.016507962718605995\n",
      "epoch:  1825 loss:  0.01646297238767147\n",
      "epoch:  1830 loss:  0.01641831174492836\n",
      "epoch:  1835 loss:  0.016373958438634872\n",
      "epoch:  1840 loss:  0.016329923644661903\n",
      "epoch:  1845 loss:  0.016286201775074005\n",
      "epoch:  1850 loss:  0.016242794692516327\n",
      "epoch:  1855 loss:  0.016199694946408272\n",
      "epoch:  1860 loss:  0.01615690439939499\n",
      "epoch:  1865 loss:  0.01611442305147648\n",
      "epoch:  1870 loss:  0.016072245314717293\n",
      "epoch:  1875 loss:  0.016030369326472282\n",
      "epoch:  1880 loss:  0.015988800674676895\n",
      "epoch:  1885 loss:  0.015947535634040833\n",
      "epoch:  1890 loss:  0.015906568616628647\n",
      "epoch:  1895 loss:  0.015865899622440338\n",
      "epoch:  1900 loss:  0.01582552306354046\n",
      "epoch:  1905 loss:  0.01578543893992901\n",
      "epoch:  1910 loss:  0.01574564166367054\n",
      "epoch:  1915 loss:  0.015706133097410202\n",
      "epoch:  1920 loss:  0.015666905790567398\n",
      "epoch:  1925 loss:  0.015627961605787277\n",
      "epoch:  1930 loss:  0.015589282847940922\n",
      "epoch:  1935 loss:  0.01555086113512516\n",
      "epoch:  1940 loss:  0.015512676909565926\n",
      "epoch:  1945 loss:  0.015474721789360046\n",
      "epoch:  1950 loss:  0.015436994843184948\n",
      "epoch:  1955 loss:  0.015399494208395481\n",
      "epoch:  1960 loss:  0.015362230129539967\n",
      "epoch:  1965 loss:  0.015325197950005531\n",
      "epoch:  1970 loss:  0.015288381837308407\n",
      "epoch:  1975 loss:  0.01525174267590046\n",
      "epoch:  1980 loss:  0.015215235762298107\n",
      "epoch:  1985 loss:  0.015178823843598366\n",
      "epoch:  1990 loss:  0.015142523683607578\n",
      "epoch:  1995 loss:  0.015106398612260818\n",
      "epoch:  2000 loss:  0.015070478431880474\n"
     ]
    }
   ],
   "source": [
    "ls.train_model(layers=(10,10),verbose=True,n_iters = 2000)  ## ANN with input_len * 10 * 10 * 4 (# of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26e2a0",
   "metadata": {},
   "source": [
    "## 4. Checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee26cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.99\n"
     ]
    }
   ],
   "source": [
    "ls.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655aad6",
   "metadata": {},
   "source": [
    "### 4.1 checking predictions for only these examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc2a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [data[0][0:4000],data[3][10:4010]]\n",
    "output = ls.predict(X_test)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
