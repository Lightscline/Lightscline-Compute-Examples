{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a818ae7",
   "metadata": {},
   "source": [
    "### TLDR:\n",
    "\n",
    "1. Download and install the software. \n",
    "2. Import your data\n",
    "5. Train your model \n",
    "6. See the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51e1d0",
   "metadata": {},
   "source": [
    "## 1. Download and Install the software\n",
    "\n",
    "### 1.1 Download the software \n",
    "\n",
    "\n",
    "1. Register yourself via <a href=\"https://forms.gle/ND5hFbznqmyVQKaR6\" target=\"_blank\">google Form</a>.\n",
    "2. Within 24 hrs, You will get the link to download our software free for 21 days (no credit card required)\n",
    "\n",
    "### 1.2 Install our software\n",
    "0. (optional) Create a virtual environment \n",
    " - run `python -m venv env` to create a new virtual environment. This should create a folder named env.\n",
    " - run `.\\env\\Scripts\\activate` to activate the virtual environment.\n",
    " - (optional) install jupyter notebook for iteractive session `pip install jupyter`\n",
    " \n",
    " \n",
    "1. Install lightscline package by `pip install <path to the .tar.gz file>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d250bf",
   "metadata": {},
   "source": [
    "### 1.3 Import the package to check if it is correctly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79bab9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a free version with limited capabilities.\n",
      "Please contact us at info@lightscline.com for the full version. Our team will guide you through the complete suite of functionalities, tailored for your specific needs.\n"
     ]
    }
   ],
   "source": [
    "from lightscline.lightscline import LightsclineEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fec7e",
   "metadata": {},
   "source": [
    "## 2. Import your data\n",
    "\n",
    "You can either import your data or use a dummy dataset to see how it work. \n",
    "\n",
    "Dummy data can be created by typing `data = [[0 for i in range(10000)], [1 for i in range(8000)]]` in python. This will create 2 classes ( two timeseries columns of unequal lengths). Each column represents one class, unless speficified otherwise. Below is a example for getting the data into classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3930941",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation for ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526b411",
   "metadata": {},
   "source": [
    "This example uses \n",
    "<a href=\"https://engineering.case.edu/bearingdatacenter\" target=\"_blank\">Case Western Reserve University bearing fault dataset</a>.\n",
    "Each of the .mat files are 4 different classes (3 different types of bearing fault + 1 normal bearing). The data for each class is a time series vibration data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe0b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify where the data is located. \n",
    "data_folder = r\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfcc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "## load the data\n",
    "mat1 = scipy.io.loadmat(f'{data_folder}/Normal_1730_1.mat')\n",
    "mat2 = scipy.io.loadmat(f'{data_folder}/B007_1_DE.mat')\n",
    "mat3 = scipy.io.loadmat(f'{data_folder}/IR007_1_DE.mat')\n",
    "mat4 = scipy.io.loadmat(f'{data_folder}/OR007_1_DE.mat')\n",
    "\n",
    "## Convert the data of each file into a list \n",
    "normal_data = mat1['X100_DE_time'].reshape(-1).tolist()\n",
    "ball_defect = mat2['X121_DE_time'].reshape(-1).tolist()\n",
    "IR_defect = mat3['X108_DE_time'].reshape(-1).tolist()\n",
    "OR_defect = mat4['X133_DE_time'].reshape(-1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4404041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the data into desired format.\n",
    "data = [normal_data[:100000],ball_defect[:100000], IR_defect[:100000], OR_defect[:100000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627dc9c",
   "metadata": {},
   "source": [
    "### 2.2 Ingest your data into LightsclineEdge package\n",
    "\n",
    "Once the dataset is in form of List of List, we can import it into LightsclineEdge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feea6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LightsclineEdge(data=data,fs = 12000,labels = None)  ## Labels will be decided based on column number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f1d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will create windows of 4000<window_size> points each and reduce each window size by 90% to train the data on trained windown size.\n",
    "ls.reduce_and_preprocess_data(per_reduction=90) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42513e",
   "metadata": {},
   "source": [
    "## 3. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e284da",
   "metadata": {},
   "source": [
    "You can change the number of layers to find the best model for your data. More advanced finetuning can be done by contacting us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a080e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 loss:  1.3808163404464722\n",
      "epoch:  10 loss:  1.3592787981033325\n",
      "epoch:  15 loss:  1.337395191192627\n",
      "epoch:  20 loss:  1.3147865533828735\n",
      "epoch:  25 loss:  1.2912449836730957\n",
      "epoch:  30 loss:  1.2662694454193115\n",
      "epoch:  35 loss:  1.2393569946289062\n",
      "epoch:  40 loss:  1.2105597257614136\n",
      "epoch:  45 loss:  1.1802932024002075\n",
      "epoch:  50 loss:  1.1489101648330688\n",
      "epoch:  55 loss:  1.116869330406189\n",
      "epoch:  60 loss:  1.0846643447875977\n",
      "epoch:  65 loss:  1.052807092666626\n",
      "epoch:  70 loss:  1.0215870141983032\n",
      "epoch:  75 loss:  0.9910948276519775\n",
      "epoch:  80 loss:  0.9613021612167358\n",
      "epoch:  85 loss:  0.9321783185005188\n",
      "epoch:  90 loss:  0.9037030935287476\n",
      "epoch:  95 loss:  0.8758538961410522\n",
      "epoch:  100 loss:  0.8485658168792725\n",
      "epoch:  105 loss:  0.821931004524231\n",
      "epoch:  110 loss:  0.7961922287940979\n",
      "epoch:  115 loss:  0.7714475989341736\n",
      "epoch:  120 loss:  0.7476658821105957\n",
      "epoch:  125 loss:  0.7247909307479858\n",
      "epoch:  130 loss:  0.7027997374534607\n",
      "epoch:  135 loss:  0.6817061901092529\n",
      "epoch:  140 loss:  0.6614871621131897\n",
      "epoch:  145 loss:  0.6420869827270508\n",
      "epoch:  150 loss:  0.6234598159790039\n",
      "epoch:  155 loss:  0.605564534664154\n",
      "epoch:  160 loss:  0.5883687138557434\n",
      "epoch:  165 loss:  0.5718297958374023\n",
      "epoch:  170 loss:  0.5559287071228027\n",
      "epoch:  175 loss:  0.5406566262245178\n",
      "epoch:  180 loss:  0.5259944200515747\n",
      "epoch:  185 loss:  0.5118785500526428\n",
      "epoch:  190 loss:  0.49826252460479736\n",
      "epoch:  195 loss:  0.4851223826408386\n",
      "epoch:  200 loss:  0.4724457561969757\n",
      "epoch:  205 loss:  0.4602370262145996\n",
      "epoch:  210 loss:  0.4484942555427551\n",
      "epoch:  215 loss:  0.43718886375427246\n",
      "epoch:  220 loss:  0.4262887239456177\n",
      "epoch:  225 loss:  0.4157884418964386\n",
      "epoch:  230 loss:  0.4056834280490875\n",
      "epoch:  235 loss:  0.3959586024284363\n",
      "epoch:  240 loss:  0.3865954577922821\n",
      "epoch:  245 loss:  0.3775775730609894\n",
      "epoch:  250 loss:  0.3688921332359314\n",
      "epoch:  255 loss:  0.36052802205085754\n",
      "epoch:  260 loss:  0.35248154401779175\n",
      "epoch:  265 loss:  0.3447607457637787\n",
      "epoch:  270 loss:  0.3373591899871826\n",
      "epoch:  275 loss:  0.3302508592605591\n",
      "epoch:  280 loss:  0.3234127163887024\n",
      "epoch:  285 loss:  0.3168267011642456\n",
      "epoch:  290 loss:  0.310474693775177\n",
      "epoch:  295 loss:  0.3043464124202728\n",
      "epoch:  300 loss:  0.29843777418136597\n",
      "epoch:  305 loss:  0.29274994134902954\n",
      "epoch:  310 loss:  0.28728288412094116\n",
      "epoch:  315 loss:  0.2820255160331726\n",
      "epoch:  320 loss:  0.2769601345062256\n",
      "epoch:  325 loss:  0.27206745743751526\n",
      "epoch:  330 loss:  0.2673303186893463\n",
      "epoch:  335 loss:  0.2627390921115875\n",
      "epoch:  340 loss:  0.25828632712364197\n",
      "epoch:  345 loss:  0.25396299362182617\n",
      "epoch:  350 loss:  0.24977584183216095\n",
      "epoch:  355 loss:  0.24572418630123138\n",
      "epoch:  360 loss:  0.24180352687835693\n",
      "epoch:  365 loss:  0.23800624907016754\n",
      "epoch:  370 loss:  0.23432376980781555\n",
      "epoch:  375 loss:  0.23075130581855774\n",
      "epoch:  380 loss:  0.22728468477725983\n",
      "epoch:  385 loss:  0.22391681373119354\n",
      "epoch:  390 loss:  0.22063976526260376\n",
      "epoch:  395 loss:  0.21745014190673828\n",
      "epoch:  400 loss:  0.21434849500656128\n",
      "epoch:  405 loss:  0.21133536100387573\n",
      "epoch:  410 loss:  0.2083994597196579\n",
      "epoch:  415 loss:  0.20553770661354065\n",
      "epoch:  420 loss:  0.2027619183063507\n",
      "epoch:  425 loss:  0.20005954802036285\n",
      "epoch:  430 loss:  0.1974269151687622\n",
      "epoch:  435 loss:  0.1948653906583786\n",
      "epoch:  440 loss:  0.19238021969795227\n",
      "epoch:  445 loss:  0.18996790051460266\n",
      "epoch:  450 loss:  0.1876215636730194\n",
      "epoch:  455 loss:  0.18533030152320862\n",
      "epoch:  460 loss:  0.1830999106168747\n",
      "epoch:  465 loss:  0.1809232234954834\n",
      "epoch:  470 loss:  0.17878757417201996\n",
      "epoch:  475 loss:  0.176692396402359\n",
      "epoch:  480 loss:  0.17463405430316925\n",
      "epoch:  485 loss:  0.17261295020580292\n",
      "epoch:  490 loss:  0.17063477635383606\n",
      "epoch:  495 loss:  0.16870234906673431\n",
      "epoch:  500 loss:  0.16681158542633057\n",
      "epoch:  505 loss:  0.16496075689792633\n",
      "epoch:  510 loss:  0.16314564645290375\n",
      "epoch:  515 loss:  0.16135799884796143\n",
      "epoch:  520 loss:  0.15958809852600098\n",
      "epoch:  525 loss:  0.15782994031906128\n",
      "epoch:  530 loss:  0.1560894250869751\n",
      "epoch:  535 loss:  0.15437918901443481\n",
      "epoch:  540 loss:  0.15268930792808533\n",
      "epoch:  545 loss:  0.15102054178714752\n",
      "epoch:  550 loss:  0.14939621090888977\n",
      "epoch:  555 loss:  0.1478109359741211\n",
      "epoch:  560 loss:  0.14625941216945648\n",
      "epoch:  565 loss:  0.1447359323501587\n",
      "epoch:  570 loss:  0.14323334395885468\n",
      "epoch:  575 loss:  0.14175058901309967\n",
      "epoch:  580 loss:  0.14029714465141296\n",
      "epoch:  585 loss:  0.13888195157051086\n",
      "epoch:  590 loss:  0.13750192523002625\n",
      "epoch:  595 loss:  0.13614565134048462\n",
      "epoch:  600 loss:  0.1348143368959427\n",
      "epoch:  605 loss:  0.13352495431900024\n",
      "epoch:  610 loss:  0.13222455978393555\n",
      "epoch:  615 loss:  0.13096550107002258\n",
      "epoch:  620 loss:  0.12972262501716614\n",
      "epoch:  625 loss:  0.12849202752113342\n",
      "epoch:  630 loss:  0.12727336585521698\n",
      "epoch:  635 loss:  0.12606102228164673\n",
      "epoch:  640 loss:  0.1248503178358078\n",
      "epoch:  645 loss:  0.1236475482583046\n",
      "epoch:  650 loss:  0.12244654446840286\n",
      "epoch:  655 loss:  0.1212598904967308\n",
      "epoch:  660 loss:  0.12010882049798965\n",
      "epoch:  665 loss:  0.11898988485336304\n",
      "epoch:  670 loss:  0.11789463460445404\n",
      "epoch:  675 loss:  0.1168132871389389\n",
      "epoch:  680 loss:  0.11574424803256989\n",
      "epoch:  685 loss:  0.11468632519245148\n",
      "epoch:  690 loss:  0.11364439874887466\n",
      "epoch:  695 loss:  0.11261442303657532\n",
      "epoch:  700 loss:  0.11159086227416992\n",
      "epoch:  705 loss:  0.11058282852172852\n",
      "epoch:  710 loss:  0.10958969593048096\n",
      "epoch:  715 loss:  0.10860972106456757\n",
      "epoch:  720 loss:  0.10764707624912262\n",
      "epoch:  725 loss:  0.10669959336519241\n",
      "epoch:  730 loss:  0.10575803369283676\n",
      "epoch:  735 loss:  0.10481368005275726\n",
      "epoch:  740 loss:  0.10388323664665222\n",
      "epoch:  745 loss:  0.102984219789505\n",
      "epoch:  750 loss:  0.10210972279310226\n",
      "epoch:  755 loss:  0.10125469416379929\n",
      "epoch:  760 loss:  0.10041940957307816\n",
      "epoch:  765 loss:  0.09960255771875381\n",
      "epoch:  770 loss:  0.09880117326974869\n",
      "epoch:  775 loss:  0.09801437705755234\n",
      "epoch:  780 loss:  0.09724299609661102\n",
      "epoch:  785 loss:  0.09648244082927704\n",
      "epoch:  790 loss:  0.0957266241312027\n",
      "epoch:  795 loss:  0.09497050195932388\n",
      "epoch:  800 loss:  0.09420883655548096\n",
      "epoch:  805 loss:  0.09344280511140823\n",
      "epoch:  810 loss:  0.09269960969686508\n",
      "epoch:  815 loss:  0.0918668583035469\n",
      "epoch:  820 loss:  0.09108792990446091\n",
      "epoch:  825 loss:  0.09032855927944183\n",
      "epoch:  830 loss:  0.08958053588867188\n",
      "epoch:  835 loss:  0.08883760869503021\n",
      "epoch:  840 loss:  0.08810379356145859\n",
      "epoch:  845 loss:  0.08737479150295258\n",
      "epoch:  850 loss:  0.08664605766534805\n",
      "epoch:  855 loss:  0.0859241709113121\n",
      "epoch:  860 loss:  0.08521364629268646\n",
      "epoch:  865 loss:  0.08450473845005035\n",
      "epoch:  870 loss:  0.08379694819450378\n",
      "epoch:  875 loss:  0.08310855180025101\n",
      "epoch:  880 loss:  0.08242525905370712\n",
      "epoch:  885 loss:  0.08174213767051697\n",
      "epoch:  890 loss:  0.08105863630771637\n",
      "epoch:  895 loss:  0.08038094639778137\n",
      "epoch:  900 loss:  0.07968569546937943\n",
      "epoch:  905 loss:  0.0789908915758133\n",
      "epoch:  910 loss:  0.07830318063497543\n",
      "epoch:  915 loss:  0.07761313021183014\n",
      "epoch:  920 loss:  0.07693565636873245\n",
      "epoch:  925 loss:  0.07627671957015991\n",
      "epoch:  930 loss:  0.075643390417099\n",
      "epoch:  935 loss:  0.07502321898937225\n",
      "epoch:  940 loss:  0.07441335171461105\n",
      "epoch:  945 loss:  0.07382083684206009\n",
      "epoch:  950 loss:  0.07324502617120743\n",
      "epoch:  955 loss:  0.07267767190933228\n",
      "epoch:  960 loss:  0.07212286442518234\n",
      "epoch:  965 loss:  0.07157900184392929\n",
      "epoch:  970 loss:  0.07104334980249405\n",
      "epoch:  975 loss:  0.07051577419042587\n",
      "epoch:  980 loss:  0.06999613344669342\n",
      "epoch:  985 loss:  0.06949079781770706\n",
      "epoch:  990 loss:  0.06899290531873703\n",
      "epoch:  995 loss:  0.06848783046007156\n",
      "epoch:  1000 loss:  0.06799127161502838\n",
      "epoch:  1005 loss:  0.06750724464654922\n",
      "epoch:  1010 loss:  0.06703261286020279\n",
      "epoch:  1015 loss:  0.06657224148511887\n",
      "epoch:  1020 loss:  0.06612533330917358\n",
      "epoch:  1025 loss:  0.06568850576877594\n",
      "epoch:  1030 loss:  0.06526099890470505\n",
      "epoch:  1035 loss:  0.06484254449605942\n",
      "epoch:  1040 loss:  0.06442964822053909\n",
      "epoch:  1045 loss:  0.06401821225881577\n",
      "epoch:  1050 loss:  0.06360577791929245\n",
      "epoch:  1055 loss:  0.06319160759449005\n",
      "epoch:  1060 loss:  0.06277986615896225\n",
      "epoch:  1065 loss:  0.06237127631902695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1070 loss:  0.06196014583110809\n",
      "epoch:  1075 loss:  0.061543624848127365\n",
      "epoch:  1080 loss:  0.06112256273627281\n",
      "epoch:  1085 loss:  0.060696691274642944\n",
      "epoch:  1090 loss:  0.06028169021010399\n",
      "epoch:  1095 loss:  0.05987269803881645\n",
      "epoch:  1100 loss:  0.05946546792984009\n",
      "epoch:  1105 loss:  0.05906740576028824\n",
      "epoch:  1110 loss:  0.05866917222738266\n",
      "epoch:  1115 loss:  0.05827347934246063\n",
      "epoch:  1120 loss:  0.057882457971572876\n",
      "epoch:  1125 loss:  0.057494036853313446\n",
      "epoch:  1130 loss:  0.057112205773591995\n",
      "epoch:  1135 loss:  0.056730326265096664\n",
      "epoch:  1140 loss:  0.05634526163339615\n",
      "epoch:  1145 loss:  0.05595973879098892\n",
      "epoch:  1150 loss:  0.05557568371295929\n",
      "epoch:  1155 loss:  0.0551946647465229\n",
      "epoch:  1160 loss:  0.05482005700469017\n",
      "epoch:  1165 loss:  0.054450422525405884\n",
      "epoch:  1170 loss:  0.05408548191189766\n",
      "epoch:  1175 loss:  0.05372689291834831\n",
      "epoch:  1180 loss:  0.053375862538814545\n",
      "epoch:  1185 loss:  0.05303209647536278\n",
      "epoch:  1190 loss:  0.05269494652748108\n",
      "epoch:  1195 loss:  0.05236338824033737\n",
      "epoch:  1200 loss:  0.05203760415315628\n",
      "epoch:  1205 loss:  0.05171768367290497\n",
      "epoch:  1210 loss:  0.05140159651637077\n",
      "epoch:  1215 loss:  0.05110134929418564\n",
      "epoch:  1220 loss:  0.05082998052239418\n",
      "epoch:  1225 loss:  0.05051055923104286\n",
      "epoch:  1230 loss:  0.050218455493450165\n",
      "epoch:  1235 loss:  0.04993584007024765\n",
      "epoch:  1240 loss:  0.04965527728199959\n",
      "epoch:  1245 loss:  0.04937596991658211\n",
      "epoch:  1250 loss:  0.04909816011786461\n",
      "epoch:  1255 loss:  0.048821911215782166\n",
      "epoch:  1260 loss:  0.04854746162891388\n",
      "epoch:  1265 loss:  0.04827519878745079\n",
      "epoch:  1270 loss:  0.04800606518983841\n",
      "epoch:  1275 loss:  0.04774104058742523\n",
      "epoch:  1280 loss:  0.04747999832034111\n",
      "epoch:  1285 loss:  0.0472220852971077\n",
      "epoch:  1290 loss:  0.04696587845683098\n",
      "epoch:  1295 loss:  0.04671081155538559\n",
      "epoch:  1300 loss:  0.04645831882953644\n",
      "epoch:  1305 loss:  0.04620764032006264\n",
      "epoch:  1310 loss:  0.04595893621444702\n",
      "epoch:  1315 loss:  0.04571237415075302\n",
      "epoch:  1320 loss:  0.04546637088060379\n",
      "epoch:  1325 loss:  0.04521834850311279\n",
      "epoch:  1330 loss:  0.04497283697128296\n",
      "epoch:  1335 loss:  0.044732555747032166\n",
      "epoch:  1340 loss:  0.044496115297079086\n",
      "epoch:  1345 loss:  0.04426170885562897\n",
      "epoch:  1350 loss:  0.044025178998708725\n",
      "epoch:  1355 loss:  0.043783269822597504\n",
      "epoch:  1360 loss:  0.04354435205459595\n",
      "epoch:  1365 loss:  0.04331190139055252\n",
      "epoch:  1370 loss:  0.0430879183113575\n",
      "epoch:  1375 loss:  0.04287268593907356\n",
      "epoch:  1380 loss:  0.04266268014907837\n",
      "epoch:  1385 loss:  0.042454566806554794\n",
      "epoch:  1390 loss:  0.04224666580557823\n",
      "epoch:  1395 loss:  0.042039405554533005\n",
      "epoch:  1400 loss:  0.04183421656489372\n",
      "epoch:  1405 loss:  0.04163150116801262\n",
      "epoch:  1410 loss:  0.04143011197447777\n",
      "epoch:  1415 loss:  0.04122988507151604\n",
      "epoch:  1420 loss:  0.041032519191503525\n",
      "epoch:  1425 loss:  0.04083815962076187\n",
      "epoch:  1430 loss:  0.04064622148871422\n",
      "epoch:  1435 loss:  0.04046143963932991\n",
      "epoch:  1440 loss:  0.0403168685734272\n",
      "epoch:  1445 loss:  0.040086306631565094\n",
      "epoch:  1450 loss:  0.039895519614219666\n",
      "epoch:  1455 loss:  0.03971237689256668\n",
      "epoch:  1460 loss:  0.039529070258140564\n",
      "epoch:  1465 loss:  0.03934430330991745\n",
      "epoch:  1470 loss:  0.039153486490249634\n",
      "epoch:  1475 loss:  0.03895081579685211\n",
      "epoch:  1480 loss:  0.0387423112988472\n",
      "epoch:  1485 loss:  0.03853829205036163\n",
      "epoch:  1490 loss:  0.03834177181124687\n",
      "epoch:  1495 loss:  0.03814954683184624\n",
      "epoch:  1500 loss:  0.03795908764004707\n",
      "epoch:  1505 loss:  0.03777040168642998\n",
      "epoch:  1510 loss:  0.03758195787668228\n",
      "epoch:  1515 loss:  0.0373949334025383\n",
      "epoch:  1520 loss:  0.037211883813142776\n",
      "epoch:  1525 loss:  0.037031013518571854\n",
      "epoch:  1530 loss:  0.03684936463832855\n",
      "epoch:  1535 loss:  0.03667030483484268\n",
      "epoch:  1540 loss:  0.03649463132023811\n",
      "epoch:  1545 loss:  0.03631974011659622\n",
      "epoch:  1550 loss:  0.03614456579089165\n",
      "epoch:  1555 loss:  0.035970091819763184\n",
      "epoch:  1560 loss:  0.035796646028757095\n",
      "epoch:  1565 loss:  0.035623952746391296\n",
      "epoch:  1570 loss:  0.03545256704092026\n",
      "epoch:  1575 loss:  0.03528336063027382\n",
      "epoch:  1580 loss:  0.035116296261548996\n",
      "epoch:  1585 loss:  0.03494975343346596\n",
      "epoch:  1590 loss:  0.03478219732642174\n",
      "epoch:  1595 loss:  0.034613918513059616\n",
      "epoch:  1600 loss:  0.0344468429684639\n",
      "epoch:  1605 loss:  0.03428255766630173\n",
      "epoch:  1610 loss:  0.034121640026569366\n",
      "epoch:  1615 loss:  0.03396424278616905\n",
      "epoch:  1620 loss:  0.03381073847413063\n",
      "epoch:  1625 loss:  0.03366084396839142\n",
      "epoch:  1630 loss:  0.033513158559799194\n",
      "epoch:  1635 loss:  0.03336570784449577\n",
      "epoch:  1640 loss:  0.03321674466133118\n",
      "epoch:  1645 loss:  0.0330660417675972\n",
      "epoch:  1650 loss:  0.032915521413087845\n",
      "epoch:  1655 loss:  0.032766807824373245\n",
      "epoch:  1660 loss:  0.032618843019008636\n",
      "epoch:  1665 loss:  0.03246907889842987\n",
      "epoch:  1670 loss:  0.03231608867645264\n",
      "epoch:  1675 loss:  0.03215912729501724\n",
      "epoch:  1680 loss:  0.031999409198760986\n",
      "epoch:  1685 loss:  0.031840696930885315\n",
      "epoch:  1690 loss:  0.03168310225009918\n",
      "epoch:  1695 loss:  0.03152722492814064\n",
      "epoch:  1700 loss:  0.03137999773025513\n",
      "epoch:  1705 loss:  0.031242620199918747\n",
      "epoch:  1710 loss:  0.031110670417547226\n",
      "epoch:  1715 loss:  0.03098161146044731\n",
      "epoch:  1720 loss:  0.03085547499358654\n",
      "epoch:  1725 loss:  0.030733352527022362\n",
      "epoch:  1730 loss:  0.030614342540502548\n",
      "epoch:  1735 loss:  0.030495844781398773\n",
      "epoch:  1740 loss:  0.03037598915398121\n",
      "epoch:  1745 loss:  0.030255014076828957\n",
      "epoch:  1750 loss:  0.030133550986647606\n",
      "epoch:  1755 loss:  0.03001185692846775\n",
      "epoch:  1760 loss:  0.029890015721321106\n",
      "epoch:  1765 loss:  0.029767928645014763\n",
      "epoch:  1770 loss:  0.029645539820194244\n",
      "epoch:  1775 loss:  0.029521338641643524\n",
      "epoch:  1780 loss:  0.02939032018184662\n",
      "epoch:  1785 loss:  0.029246140271425247\n",
      "epoch:  1790 loss:  0.029094448313117027\n",
      "epoch:  1795 loss:  0.028940871357917786\n",
      "epoch:  1800 loss:  0.02878495119512081\n",
      "epoch:  1805 loss:  0.0286313034594059\n",
      "epoch:  1810 loss:  0.02848624624311924\n",
      "epoch:  1815 loss:  0.028340034186840057\n",
      "epoch:  1820 loss:  0.028191549703478813\n",
      "epoch:  1825 loss:  0.028034579008817673\n",
      "epoch:  1830 loss:  0.027870919555425644\n",
      "epoch:  1835 loss:  0.02770611271262169\n",
      "epoch:  1840 loss:  0.02754909358918667\n",
      "epoch:  1845 loss:  0.0273966733366251\n",
      "epoch:  1850 loss:  0.02724873647093773\n",
      "epoch:  1855 loss:  0.027110716328024864\n",
      "epoch:  1860 loss:  0.02698352374136448\n",
      "epoch:  1865 loss:  0.02686292864382267\n",
      "epoch:  1870 loss:  0.026746878400444984\n",
      "epoch:  1875 loss:  0.026634735986590385\n",
      "epoch:  1880 loss:  0.026523549109697342\n",
      "epoch:  1885 loss:  0.02641156315803528\n",
      "epoch:  1890 loss:  0.026302563026547432\n",
      "epoch:  1895 loss:  0.02619723230600357\n",
      "epoch:  1900 loss:  0.026090942323207855\n",
      "epoch:  1905 loss:  0.025980496779084206\n",
      "epoch:  1910 loss:  0.025866806507110596\n",
      "epoch:  1915 loss:  0.02575494907796383\n",
      "epoch:  1920 loss:  0.025646431371569633\n",
      "epoch:  1925 loss:  0.02553974837064743\n",
      "epoch:  1930 loss:  0.025434061884880066\n",
      "epoch:  1935 loss:  0.02532934583723545\n",
      "epoch:  1940 loss:  0.025225229561328888\n",
      "epoch:  1945 loss:  0.02512245997786522\n",
      "epoch:  1950 loss:  0.025022180750966072\n",
      "epoch:  1955 loss:  0.024927783757448196\n",
      "epoch:  1960 loss:  0.02483254298567772\n",
      "epoch:  1965 loss:  0.02472541853785515\n",
      "epoch:  1970 loss:  0.024627355858683586\n",
      "epoch:  1975 loss:  0.024537166580557823\n",
      "epoch:  1980 loss:  0.024448039010167122\n",
      "epoch:  1985 loss:  0.024362394586205482\n",
      "epoch:  1990 loss:  0.02426842227578163\n",
      "epoch:  1995 loss:  0.024171030148863792\n",
      "epoch:  2000 loss:  0.02408326417207718\n"
     ]
    }
   ],
   "source": [
    "ls.train_model(layers=(10,10),verbose=True,n_iters = 2000)  ## ANN with input_len * 10 * 10 * 4 (# of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26e2a0",
   "metadata": {},
   "source": [
    "## 4. Checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee26cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.982\n"
     ]
    }
   ],
   "source": [
    "ls.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655aad6",
   "metadata": {},
   "source": [
    "### 4.1 checking predictions for only these examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc2a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [data[0][0:4000],data[3][10:4010]]\n",
    "output = ls.predict(X_test)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
